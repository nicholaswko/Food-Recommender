{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver-manager in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (3.3.0)\n",
      "Requirement already satisfied: configparser in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from webdriver-manager) (5.0.1)\n",
      "Requirement already satisfied: requests in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from webdriver-manager) (2.24.0)\n",
      "Requirement already satisfied: crayons in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from webdriver-manager) (0.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from requests->webdriver-manager) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from requests->webdriver-manager) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from requests->webdriver-manager) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from requests->webdriver-manager) (3.0.4)\n",
      "Requirement already satisfied: colorama in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from crayons->webdriver-manager) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from selenium) (1.25.11)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /Users/jacky/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import datetime\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pickle\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options  \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import UnexpectedAlertPresentException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identification(location_data_set):\n",
    "    person = input(\"Please input your name: \").lower()\n",
    "    \n",
    "    while True:\n",
    "        if person == 'jacky':\n",
    "            return person, location_data_set[:1323]\n",
    "        elif person == 'nicholas':\n",
    "            return person, location_data_set[1323:2646]\n",
    "        elif person == 'zhou wei':\n",
    "            return person, location_data_set[2646:]\n",
    "        else:\n",
    "            print(\"Unknown Person\")\n",
    "            person = input(\"Please input your name: \").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_containers(driver, current_set):\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    list_of_places = set(soup.find_all(\"article\", class_ = \"item restaurant_item\" ))\n",
    "    return list_of_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraping(current_list_of_locations):\n",
    "    \n",
    "    # User Experience\n",
    "    print(f\"You have {len(current_list_of_locations)} left to scrape\", end = \"\\n\")\n",
    "    while True:\n",
    "        try:\n",
    "            number_of_scrapes = int(input(\"Please indicate how many locations would you like to scrape now: \"))\n",
    "        except:\n",
    "            pass\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Getting the locations to be scraped\n",
    "    list_of_locations = current_list_of_locations[:number_of_scrapes]\n",
    "    list_of_restaurants = set()\n",
    "    remaining_locations = current_list_of_locations[number_of_scrapes:]\n",
    "    \n",
    "\n",
    "    # Setting the parameters for the driver\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options = chrome_options)\n",
    "    delay = 5\n",
    "\n",
    "    \n",
    "    # Start scraping the web\n",
    "    while(len(list_of_locations) != 0):   \n",
    "        # getting the location from the list\n",
    "        location = list_of_locations.pop()\n",
    "        driver.get(f\"https://www.hungrygowhere.com/search-results/{location}/?general=1\")\n",
    "        \n",
    "        sleep(1)\n",
    "        \n",
    "        # Getting the result count\n",
    "        try:\n",
    "            result_count = BeautifulSoup(driver.page_source, \"html.parser\").find(\"span\", class_=\"result\").text\n",
    "            if int(result_count[0]) == 0:\n",
    "                print(f\"No results for {location}\")\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Sanity Check\n",
    "        print(f\"{result_count} found!\")\n",
    "        after_split = result_count.split(' ')\n",
    "        \n",
    "        # If more than 400 results, skip to next location\n",
    "        if int(after_split[0]) > 400:\n",
    "            print(f\"{location} has too many results! Passing..\")\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            print(f\"Currently Scraping: {location}\")\n",
    "\n",
    "            # Get scroll height\n",
    "            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            try:\n",
    "                while True:\n",
    "                    # Scroll down to bottom\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                    # Wait to load page\n",
    "                    sleep(delay)\n",
    "\n",
    "                    list_of_restaurants = list_of_restaurants.union(get_containers(driver, list_of_restaurants))\n",
    "                    # Calculate new scroll height and compare with last scroll height\n",
    "                    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_height == last_height:\n",
    "                        list_of_restaurants = list_of_restaurants.union(get_containers(driver, list_of_restaurants))\n",
    "                        break\n",
    "                    last_height = new_height\n",
    "            # If encounter time out error, restart the driver and just move on\n",
    "            except:\n",
    "                print(f\"Failed to scape all locations for {location}... Moving on\")\n",
    "                driver.quit()\n",
    "                driver = webdriver.Chrome(ChromeDriverManager().install(), options = chrome_options)\n",
    "                continue\n",
    "    \n",
    "    # Sanity Check\n",
    "    print(\"Scraping done!\")\n",
    "    driver.quit()\n",
    "    return list_of_restaurants, remaining_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To extract the shop name, the shop address, cuisine, type of shop(eg. hawker/cafe/restaurant)\n",
    "def data_extractor(aset):\n",
    "    # Convert set to list for iteration\n",
    "    soup_list = list(aset)\n",
    "    \n",
    "    # Preparing the container\n",
    "    big_list = []\n",
    "    \n",
    "    # looping through all soups extracted\n",
    "    for containers in soup_list:\n",
    "        # looping through all soups extracted\n",
    "        name = containers.find(\"h2\", class_ = \"hneue-bold-mm\").find(\"a\").text\n",
    "        \n",
    "        # Extracting type of cuisine\n",
    "        cuisines = []\n",
    "        try:\n",
    "            cuisine = containers.find(\"span\", class_ = \"cuisine\").find_all(\"a\")\n",
    "            for i in cuisine:\n",
    "                cuisines.append(i.text)\n",
    "        except:\n",
    "            cuisines.append(None)\n",
    "        \n",
    "        # Extracting stall type \n",
    "        try:\n",
    "            stall_type = containers.find(\"span\", class_ = \"category-name\").text\n",
    "        except:\n",
    "            stall_type = None\n",
    "        \n",
    "        # Extracting address          \n",
    "        address_container = containers.find(\"span\", class_ = \"location\")\n",
    "        try:\n",
    "            street_address = address_container.find(\"span\", itemprop = \"streetAddress\").text\n",
    "        except:\n",
    "            street_address = \"\"\n",
    "            \n",
    "        try:\n",
    "            address_locality = address_container.find(\"a\", itemprop = \"addressLocality\").text\n",
    "        except:\n",
    "            address_locality = \"\"\n",
    "        \n",
    "        try:\n",
    "            address_region = address_container.find(\"a\", itemprop = \"addressRegion\").text\n",
    "        except:\n",
    "            address_region = \"\"\n",
    "        \n",
    "        try:\n",
    "            postal_code = address_container.find(\"span\", itemprop = \"postalCode\").text\n",
    "        except:\n",
    "            postal_code = \"\"\n",
    "        \n",
    "        address = ( street_address + \" \" +\n",
    "                    address_locality + \" \" +\n",
    "                    address_region +\n",
    "                    postal_code)\n",
    "  \n",
    "        # Storing data into a overall list\n",
    "        alist = [name, cuisines, stall_type, address]\n",
    "        big_list.append(alist)\n",
    "        \n",
    "    return pd.DataFrame(big_list, columns= [\"Stall Name\", \"Cuisine\", \"Shop Type\", \"Address\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        # Attempt to open a save file. If no save file is found create a new backup\n",
    "        infile1 = open(\"remaining locations\", \"rb\")\n",
    "        remaining = pickle.load(infile1)\n",
    "        infile1.close()\n",
    "        \n",
    "        scraped = pd.read_pickle(\"scraped_data\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Initialising first time user setup...\")\n",
    "        \n",
    "        # Extracting the new users tasks to scrape\n",
    "        df = pd.read_excel(\"Street Names.xlsx\")\n",
    "        list_of_singapore_streets = list(df.values[0:,0])\n",
    "        name, locations = identification(list_of_singapore_streets)\n",
    "        \n",
    "        print()\n",
    "        print(f\"Welcome {name}!\")\n",
    "        \n",
    "        #begin scraping and \n",
    "        scraped_data, remaining = scraping(locations)\n",
    "        \n",
    "        # Cleaning the data to be able to convert to a pickle\n",
    "        scraped_data = data_extractor(scraped_data)\n",
    "        scraped_data.to_pickle(\"scraped_data\")\n",
    "        \n",
    "        # Storing remaining locations as a pickle\n",
    "        outfile2 = open(\"remaining locations\", 'wb')\n",
    "        pickle.dump(remaining, outfile2)\n",
    "        outfile2.close()\n",
    "        \n",
    "    else:\n",
    "        print(f\"Welcome back!\")\n",
    "        \n",
    "        # Continue scaping\n",
    "        scraped_data, remaining = scraping(remaining)\n",
    "        scraped_data = data_extractor(scraped_data)\n",
    "        \n",
    "        # Merging both scraped dataframes(past and now) together\n",
    "        df = scraped.append(scraped_data, ignore_index = True)\n",
    "        df.to_pickle(\"scraped_data\")\n",
    "        \n",
    "        # Updating the remaining locations to be mined list\n",
    "        infile1 = open(\"remaining locations\", \"wb\")\n",
    "        pickle.dump(remaining, infile1)\n",
    "        infile1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome back!\n",
      "You have 1323 left to scrape\n",
      "Please indicate how many locations would you like to scrape now: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 90.0.4430\n",
      "[WDM] - Get LATEST driver version for 90.0.4430\n",
      "[WDM] - Driver [/Users/jacky/.wdm/drivers/chromedriver/mac64/90.0.4430.24/chromedriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping done!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_pickle(\"Jacky's_1st_Scrape\")\n",
    "compression_opts = dict(method='zip', archive_name='1st_Scrape.csv')  \n",
    "df.to_csv('out.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_pickle(\"Jacky's_2nd_Scrape\")\n",
    "compression_opts = dict(method='zip', archive_name='2nd_Scrape.csv')  \n",
    "df.to_csv('out.zip', index=False, compression=compression_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stall Name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Shop Type</th>\n",
       "      <th>Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adam Chicken</td>\n",
       "      <td>[None]</td>\n",
       "      <td>Hawker</td>\n",
       "      <td>#01-29 Ayer Rajah Food Centre West Coast Drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NO.1 Adam's Nasi Lemak</td>\n",
       "      <td>['Asian', 'Halal', 'Malaysian']</td>\n",
       "      <td>Kiosk or Stall</td>\n",
       "      <td>#01-01 Adam Road Food Centre Adam Road 289876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nur Adam’s Delights</td>\n",
       "      <td>['Indian']</td>\n",
       "      <td>Kiosk or Stall</td>\n",
       "      <td>Stall No.3  Sembawang Road 758504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Selamat Datang Warong Pak Sapari</td>\n",
       "      <td>['Chinese', 'Malaysian']</td>\n",
       "      <td>Kiosk or Stall</td>\n",
       "      <td>#01-09 Adam Road Food Centre Adam Road 289876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apit Drinks Stall</td>\n",
       "      <td>['Drinks']</td>\n",
       "      <td>Kiosk or Stall</td>\n",
       "      <td>#01-06 Adam Road Food Centre Adam Road 289876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5658</th>\n",
       "      <td>Zai Xin Traditional Snacks</td>\n",
       "      <td>['Snacks']</td>\n",
       "      <td>Kiosk or Stall</td>\n",
       "      <td>#01-40 Ghim Moh Road Market &amp; Cooked Food Cent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5659</th>\n",
       "      <td>Summer Pavilion</td>\n",
       "      <td>['Asian', 'Cantonese', 'Chinese', 'Dim Sum', '...</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>The Ritz-Carlton, Millenia Singapore Raffles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5660</th>\n",
       "      <td>Teochew Mushroom Minced Meat Noodle</td>\n",
       "      <td>['Asian', 'Chinese', 'Teochew']</td>\n",
       "      <td>Kiosk or Stall</td>\n",
       "      <td>#01-20 Ci Yuan Hawker Centre Hougang Avenue 9 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5661</th>\n",
       "      <td>Grove Cafe (Canberra Plaza)</td>\n",
       "      <td>[None]</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>#01-18A Canberra Plaza Canberra View 750133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5662</th>\n",
       "      <td>Ciel Patisserie (Hougang)</td>\n",
       "      <td>['Desserts', 'European', 'French', 'Snacks', '...</td>\n",
       "      <td>Cafe</td>\n",
       "      <td>#01-1444  Hougang Avenue 1 530124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5663 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Stall Name  \\\n",
       "0                            Adam Chicken   \n",
       "1                  NO.1 Adam's Nasi Lemak   \n",
       "2                     Nur Adam’s Delights   \n",
       "3        Selamat Datang Warong Pak Sapari   \n",
       "4                       Apit Drinks Stall   \n",
       "...                                   ...   \n",
       "5658           Zai Xin Traditional Snacks   \n",
       "5659                      Summer Pavilion   \n",
       "5660  Teochew Mushroom Minced Meat Noodle   \n",
       "5661          Grove Cafe (Canberra Plaza)   \n",
       "5662            Ciel Patisserie (Hougang)   \n",
       "\n",
       "                                                Cuisine       Shop Type  \\\n",
       "0                                                [None]          Hawker   \n",
       "1                       ['Asian', 'Halal', 'Malaysian']  Kiosk or Stall   \n",
       "2                                            ['Indian']  Kiosk or Stall   \n",
       "3                              ['Chinese', 'Malaysian']  Kiosk or Stall   \n",
       "4                                            ['Drinks']  Kiosk or Stall   \n",
       "...                                                 ...             ...   \n",
       "5658                                         ['Snacks']  Kiosk or Stall   \n",
       "5659  ['Asian', 'Cantonese', 'Chinese', 'Dim Sum', '...      Restaurant   \n",
       "5660                    ['Asian', 'Chinese', 'Teochew']  Kiosk or Stall   \n",
       "5661                                             [None]      Restaurant   \n",
       "5662  ['Desserts', 'European', 'French', 'Snacks', '...            Cafe   \n",
       "\n",
       "                                                Address  \n",
       "0     #01-29 Ayer Rajah Food Centre West Coast Drive...  \n",
       "1         #01-01 Adam Road Food Centre Adam Road 289876  \n",
       "2                     Stall No.3  Sembawang Road 758504  \n",
       "3         #01-09 Adam Road Food Centre Adam Road 289876  \n",
       "4         #01-06 Adam Road Food Centre Adam Road 289876  \n",
       "...                                                 ...  \n",
       "5658  #01-40 Ghim Moh Road Market & Cooked Food Cent...  \n",
       "5659   The Ritz-Carlton, Millenia Singapore Raffles ...  \n",
       "5660  #01-20 Ci Yuan Hawker Centre Hougang Avenue 9 ...  \n",
       "5661        #01-18A Canberra Plaza Canberra View 750133  \n",
       "5662                  #01-1444  Hougang Avenue 1 530124  \n",
       "\n",
       "[5663 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"Jacky1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
